{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For spline (two-model) method training only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and self-defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "# Test/Train data split\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import logging\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "# os\n",
    "import os\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# random\n",
    "import random\n",
    "# math\n",
    "import math\n",
    "import scipy.interpolate\n",
    "# self-defined model\n",
    "from model_pic import *\n",
    "from model_cal import *\n",
    "from model_one import *\n",
    "import pandas as pd\n",
    "\n",
    "#PictureNet\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augamented dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self, dataset, test_train_split=0.9, val_train_split=0.1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = list(range(dataset_size))\n",
    "        test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        train_indices, self.test_indices = self.indices[:test_split], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[:validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "    \n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=64, num_workers=0):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=64, num_workers=0):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=64, num_workers=0):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=64, num_workers=0):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for PhysicsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1          2         3         4         5         6   \\\n",
      "0       15.1   5.10061  14.150364  1.343070  0.560755 -0.204328 -0.058190   \n",
      "1       15.1   5.10061  14.150364 -0.161344  0.560755 -0.204328 -0.058190   \n",
      "2       15.1   5.10061  14.150364  0.442829  0.560755 -0.204328 -0.058190   \n",
      "3       15.1   5.10061  14.150364  1.403207  0.560755 -0.204328 -0.058190   \n",
      "4       15.1   5.20122  13.846032  0.061767  0.560057 -0.206831 -0.056986   \n",
      "...      ...       ...        ...       ...       ...       ...       ...   \n",
      "109539  93.2  44.81438  42.599241  1.326978  0.438854 -0.652535  0.146285   \n",
      "109540  93.2  45.39140  43.012347  1.657659  0.451419 -0.664851  0.125092   \n",
      "109541  93.2  45.39140  43.012347  1.722307  0.451419 -0.664851  0.125092   \n",
      "109542  93.2  45.39140  43.012347  0.943215  0.451419 -0.664851  0.125092   \n",
      "109543  93.2  45.39140  43.012347  1.005774  0.451419 -0.664851  0.125092   \n",
      "\n",
      "              7         8         9   ...        15        16        17  \\\n",
      "0       0.073569 -0.004100 -0.013163  ...  1.121736  0.132903 -0.177839   \n",
      "1       0.073569 -0.004100 -0.013163  ...  1.121736  0.132903 -0.177839   \n",
      "2       0.073569 -0.004100 -0.013163  ...  1.121736  0.132903 -0.177839   \n",
      "3       0.073569 -0.004100 -0.013163  ...  1.121736  0.132903 -0.177839   \n",
      "4       0.074633 -0.004738 -0.013435  ...  1.137861  0.133661 -0.180555   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "109539  0.277451 -0.102967 -0.070687  ...  3.506179  0.268065 -0.557008   \n",
      "109540  0.276532 -0.092678 -0.067252  ...  3.454294  0.268191 -0.550456   \n",
      "109541  0.276532 -0.092678 -0.067252  ...  3.454294  0.268191 -0.550456   \n",
      "109542  0.276532 -0.092678 -0.067252  ...  3.454294  0.268191 -0.550456   \n",
      "109543  0.276532 -0.092678 -0.067252  ...  3.454294  0.268191 -0.550456   \n",
      "\n",
      "              18        19        20        21        22        23        24  \n",
      "0      -0.036451  0.026592  0.000080 -0.004749  0.001136  0.000689 -0.000318  \n",
      "1      -0.036451  0.026592  0.000080 -0.004749  0.001136  0.000689 -0.000318  \n",
      "2      -0.036451  0.026592  0.000080 -0.004749  0.001136  0.000689 -0.000318  \n",
      "3      -0.036451  0.026592  0.000080 -0.004749  0.001136  0.000689 -0.000318  \n",
      "4      -0.036399  0.027049 -0.000124 -0.004824  0.001213  0.000696 -0.000337  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "109539 -0.029782  0.079554 -0.036623 -0.012507  0.016276  0.002413 -0.003933  \n",
      "109540 -0.031602  0.080847 -0.034377 -0.013702  0.014857  0.002576 -0.003443  \n",
      "109541 -0.031602  0.080847 -0.034377 -0.013702  0.014857  0.002576 -0.003443  \n",
      "109542 -0.031602  0.080847 -0.034377 -0.013702  0.014857  0.002576 -0.003443  \n",
      "109543 -0.031602  0.080847 -0.034377 -0.013702  0.014857  0.002576 -0.003443  \n",
      "\n",
      "[109544 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "data_matrix_augmentation = pd.read_csv(\"Data_uniform/Data_aug/Spline_coef_aug.csv\", header= None)\n",
    "print(data_matrix_augmentation)\n",
    "\n",
    "# Make the droplet dataset class based on data_matrix\n",
    "class Droplet_data_set(Dataset):\n",
    "    def __init__(self,dataInput):\n",
    "        x = dataInput.iloc[0:,3:].values\n",
    "        y = dataInput.iloc[0:,0:2].values\n",
    "        # x = np.random.normal(x,0.01)\n",
    "        y0 = dataInput.iloc[:,0].values\n",
    "        y1 = dataInput.iloc[:,1].values\n",
    "        \n",
    "        # Plot surface tension histogram\n",
    "        n, bins, patches = plt.hist(x=y0, bins='auto', color='#0504aa',\n",
    "                                    alpha=0.7, rwidth=0.85)\n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.xlabel('Surface Tension[mN/m]')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Occurence of data per Surface Tension')\\\n",
    "\n",
    "        # Plot volume histogram\n",
    "        # n, bins, patches = plt.hist(x=y1, bins='auto', color='#0504aa',\n",
    "        #                             alpha=0.7, rwidth=0.85)\n",
    "        # plt.grid(axis='y', alpha=0.75)\n",
    "        # plt.xlabel('Volume[mm^3]')\n",
    "        # plt.ylabel('Frequency')\n",
    "        # plt.title('Occurence of data per Volume')\n",
    "\n",
    "        self.x_train = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "         return len(self.y_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for PictureNet\n",
    "Read image from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n"
     ]
    }
   ],
   "source": [
    "train_img = []\n",
    "data_matrix_augmentation = pd.read_csv(\"Data_uniform/Data_aug/Spline_coef_aug.csv\", header= None)\n",
    "data_image_augmentation = data_matrix_augmentation\n",
    "\n",
    "for img_name in data_image_augmentation.iloc[0:,3:].index + 1:      # modify the number depends on size of dataset\n",
    "    # defining the image path\n",
    "    image_path = 'Data_uniform/Data_aug/' + str(img_name) + '.png'      # modify the path depends on which dataset\n",
    "    # reading the image\n",
    "    img = imread(image_path, as_gray=True)\n",
    "    img = img[4:-4, 4:-4]  #crop \n",
    "    # normalizing the pixel values\n",
    "    img /= 255.0\n",
    "    # converting the type of pixel to float 32\n",
    "    img = img.astype('float32')\n",
    "    if img_name%1000 ==0:\n",
    "        print(img_name)\n",
    "    img = np.array(img)\n",
    "    # appending the image into the list\n",
    "    train_img.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a single image for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image = 9392\n",
      "Picture size = (82, 77)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de39c6d100>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" rand = random.randint(1, 100000)\n",
    "print(\"Current image =\", rand)\n",
    "print(\"Picture size =\", train_img[rand].shape)\n",
    "plt.imshow(train_img[rand], cmap='gray')\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and transfer the data from list to torch format\n",
    "def split_n_transfer(x, y):\n",
    "    # Points data split\n",
    "    train_x_pic, rem_x_pic, train_y_pic, rem_y_pic = train_test_split(x, y, test_size = 0.2)    # default shuffle = True\n",
    "    val_x_pic, test_x_pic, val_y_pic, test_y_pic = train_test_split(rem_x_pic, rem_y_pic, test_size = 0.5)\n",
    "\n",
    "    # converting the train images of points and targets into torch format\n",
    "    train_x_pic = train_x_pic.reshape(len(train_x_pic), 1, 82, 77)\n",
    "    train_x_pic = torch.from_numpy(train_x_pic)\n",
    "    train_y_pic = torch.from_numpy(train_y_pic)\n",
    "\n",
    "    # converting the val images and target points into torch format\n",
    "    val_x_pic = val_x_pic.reshape(len(val_x_pic), 1, 82, 77)\n",
    "    val_x_pic = torch.from_numpy(val_x_pic)\n",
    "    val_y_pic = torch.from_numpy(val_y_pic)\n",
    "    # print(val_x_pic.shape, val_y_pic.shape)\n",
    "\n",
    "    # converting the test images and targets into torch format for points\n",
    "    test_x_pic = test_x_pic.reshape(len(test_x_pic), 1, 82, 77)\n",
    "    test_x_pic  = torch.from_numpy(test_x_pic)\n",
    "    test_y_pic = torch.from_numpy(test_y_pic)\n",
    "    # print(test_x_pic.shape, test_y_pic.shape)\n",
    "    \n",
    "    return train_x_pic, train_y_pic, val_x_pic, val_y_pic, test_x_pic, test_y_pic\n",
    "\n",
    "# converting the dataset into mini-batch dataset (input 4D -> 5D and 2D -> 3D)\n",
    "def batch_transform(dataset, batch_size=512):\n",
    "    batch = []\n",
    "    batch_num = math.ceil(len(dataset)/batch_size)\n",
    "    for i in range(batch_num):\n",
    "        if len(dataset) == 4:\n",
    "            if i < batch_num-1:\n",
    "                batch.append(dataset[i*batch_size:(i+1)*batch_size, :, :, :])\n",
    "            else:\n",
    "                batch.append(dataset[i*batch_size:, :, :, :])\n",
    "        else:\n",
    "            if i < batch_num-1:\n",
    "                batch.append(dataset[i*batch_size:(i+1)*batch_size, :])\n",
    "            else:\n",
    "                batch.append(dataset[i*batch_size:, :])\n",
    "    return batch\n",
    "\n",
    "# converting all raw datasets into batch datasets in the same time\n",
    "def batch_all(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    train_x_pic = batch_transform(train_x)\n",
    "    train_y_pic = batch_transform(train_y)\n",
    "    val_x_pic = batch_transform(val_x)\n",
    "    val_y_pic = batch_transform(val_y)\n",
    "    test_x_pic = batch_transform(test_x)\n",
    "    test_y_pic = batch_transform(test_y)\n",
    "    return train_x_pic, train_y_pic, val_x_pic, val_y_pic, test_x_pic, test_y_pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and splite the dataset for edge point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list to numpy array\n",
    "#train_x_pic = np.array(train_img)\n",
    "train_y_pic_augmentation = data_image_augmentation.iloc[0:,3:].values\n",
    "\n",
    "train_x_pic_augmentation, train_y_pic_augmentation, val_x_pic_augmentation, val_y_pic_augmentation, test_x_pic_augmentation, test_y_pic_augmentation = \\\n",
    "    split_n_transfer(train_x_pic, train_y_pic_augmentation)\n",
    "\n",
    "train_x_pic_augmentation, train_y_pic_augmentation, val_x_pic_augmentation, val_y_pic_augmentation, test_x_pic_augmentation, test_y_pic_augmentation = \\\n",
    "    batch_all(train_x_pic_augmentation, train_y_pic_augmentation, val_x_pic_augmentation, val_y_pic_augmentation, test_x_pic_augmentation, test_y_pic_augmentation)\n",
    "    \n",
    "print(len(train_x_pic_augmentation), len(train_y_pic_augmentation), len(val_x_pic_augmentation), \\\n",
    "    len(val_y_pic_augmentation), len(test_x_pic_augmentation), len(test_y_pic_augmentation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deside to apply host or device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal_phiysics(loss_list, model, loader):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "            loss_epoch += loss\n",
    "        loss_list.append(loss_epoch.cpu().data.numpy()/t)\n",
    "\n",
    "def loss_cal_picture(loss_list, model, x_dataset, y_dataset):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for t in range(len(x_dataset)):\n",
    "            x = x_dataset[t]\n",
    "            y = y_dataset[t]\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "            loss_epoch += loss\n",
    "        loss_list.append(loss_epoch.cpu().data.numpy()/t)\n",
    "\n",
    "def prediction(model, x):   # only a batch\n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        score = torch.Tensor.cpu(model(x))\n",
    "    return score\n",
    "\n",
    "def model_selection(model, path, loss, epoch_current, epoch_ref, loss_ref):\n",
    "    loss_best = loss_ref\n",
    "    epoch_best = epoch_ref\n",
    "    if epoch_current == 0 and ~(os.path.isdir(path)):\n",
    "            loss_current = loss[epoch_current]\n",
    "            loss_best = loss[epoch_current]\n",
    "            torch.save(model, path)\n",
    "    else:\n",
    "        loss_current = loss[epoch_current]\n",
    "        if loss_current <= (loss_best * 1.1):   # 10% margin to get better fitting but a bit more loss\n",
    "            loss_best = loss_current\n",
    "            epoch_best = epoch_current\n",
    "            torch.save(model, path)\n",
    "    return loss_best, epoch_best\n",
    "\n",
    "def train_physics(model, optimizer, epochs=1, model_input=0):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    loss_best = 0\n",
    "    epoch_best = 0\n",
    "    \n",
    "    # model selection depends on data type\n",
    "    if model_input == 0:\n",
    "        train_loader = train_loader_points\n",
    "        val_loader = val_loader_points\n",
    "        PATH_CAL = os.path.join(os.getcwd(), \"model_cal_points.pt\")\n",
    "    elif model_input == 1:\n",
    "        train_loader = train_loader\n",
    "        val_loader = val_loader\n",
    "        PATH_CAL = os.path.join(os.getcwd(), \"model_cal_spline.pt\")\n",
    "    else:\n",
    "        ValueError(\"Incorrect input: 0 for edge point model, 1 for spline model\")\n",
    "\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for _, (x, y) in enumerate(train_loader):\n",
    "            x = x.reshape(len(x), 1, 1, -1)     # reshape to 4D data for formal input of model\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_cal_phiysics(loss_train, model, train_loader)\n",
    "        loss_cal_phiysics(loss_val, model, val_loader)\n",
    "        print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "\n",
    "        # selecting the best model to save\n",
    "        loss_best, epoch_best = model_selection(model, PATH_CAL, loss_train, e, epoch_best, loss_best)\n",
    "    \n",
    "    print(\"The model is saved at epoch =\", epoch_best+1, \"and the loss =\", loss_best.item())\n",
    "    \n",
    "    x = range(1, epochs+1)\n",
    "\n",
    "    plt.semilogy(x, loss_train, 'b-', label=\"training loss\")\n",
    "    plt.semilogy(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_picture(model, optimizer, epochs=1, model_input=0):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    loss_best = 0\n",
    "    epoch_best = 0\n",
    "\n",
    "    # model selection depends on data type\n",
    "    \n",
    "    train_x_pic = train_x_pic_augmentation\n",
    "    train_y_pic = train_y_pic_augmentation\n",
    "    val_x_pic = val_x_pic_augmentation\n",
    "    val_y_pic = val_y_pic_augmentation\n",
    "    PATH_PIC = os.path.join(os.getcwd(), \"model_pic_spline.pt\")\n",
    "\n",
    "\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t in range(len(train_x_pic)):\n",
    "            model.train()  # put model to training mode\n",
    "            x = train_x_pic[t]\n",
    "            y = train_y_pic[t]\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_cal_picture(loss_train, model, train_x_pic, train_y_pic)\n",
    "        loss_cal_picture(loss_val, model, val_x_pic, val_y_pic)\n",
    "        print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "        \n",
    "        # selecting the best model to save\n",
    "        loss_best, epoch_best = model_selection(model, PATH_PIC, loss_train, e, epoch_best, loss_best)\n",
    "    \n",
    "    print(\"The model is saved at epoch =\", epoch_best+1, \"and the loss =\", loss_best.item())\n",
    "\n",
    "    x = range(1, epochs+1)\n",
    "\n",
    "    plt.semilogy(x, loss_train, 'b-', label=\"training loss\")\n",
    "    plt.semilogy(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PictureNet for spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "\"\"\" in_channel = 1\n",
    "channel_1 = 16\n",
    "channel_2 = 32\n",
    "channel_3 = 64\n",
    "node_1 = 1024\n",
    "node_2 = 1024\n",
    "out_channel = 22\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_picture = PictureNet(in_channel, channel_1, channel_2, channel_3, node_1, node_2, out_channel)\n",
    "optimizer = optim.Adam(model_picture.parameters(), lr=learning_rate)\n",
    "epoch = 200 \"\"\"\n",
    "\n",
    "#Retrain\n",
    "\n",
    "PATH_PIC = os.path.join(os.getcwd(), \"model_pic_spline.pt\")\n",
    "\n",
    "model_picture_trained = torch.load(PATH_PIC)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epoch = 200\n",
    "\n",
    "optimizer = optim.Adam(model_picture_trained.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 0 indicates edge points 1 indicates spline\n",
    "train_picture(model_picture_trained, optimizer, epoch, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d02908a6a4020c0dee71330d10741c85add751c2d2978c27222bad6a32175214"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
