{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for tensiometry of pendant drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and self-defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "#Test/Train data split\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import logging\n",
    "# os\n",
    "import os\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# self-defined model\n",
    "from model_pic import *\n",
    "from model_cal import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self, dataset, test_train_split=0.8, val_train_split=0.1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = list(range(dataset_size))\n",
    "        test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        train_indices, self.test_indices = self.indices[:test_split], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[ : validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "\n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2             3         4         5         6         7    \\\n",
      "0      2.3  0.2    1  1.507010e-26  0.012809  0.025618  0.038426  0.051234   \n",
      "1      2.4  0.2    1 -1.908640e-26  0.012808  0.025616  0.038423  0.051230   \n",
      "2      2.5  0.2    1  1.076793e-27  0.012807  0.025614  0.038421  0.051226   \n",
      "3      2.6  0.2    1 -2.257969e-27  0.012806  0.025613  0.038418  0.051223   \n",
      "4      2.7  0.2    1 -2.748220e-26  0.012806  0.025611  0.038416  0.051220   \n",
      "...    ...  ...  ...           ...       ...       ...       ...       ...   \n",
      "8215  35.3  2.1    1  3.612701e-25  0.019995  0.039982  0.059951  0.079893   \n",
      "8216  35.3  2.2    1  6.935291e-25  0.020404  0.040799  0.061176  0.081524   \n",
      "8217  35.3  2.3    1  5.082584e-25  0.020808  0.041606  0.062385  0.083133   \n",
      "8218  35.4  0.2    1  2.791266e-24  0.012794  0.025587  0.038381  0.051174   \n",
      "8219  35.4  0.3    1  1.262741e-23  0.012958  0.025916  0.038873  0.051830   \n",
      "\n",
      "           8         9    ...       155       156       157       158  \\\n",
      "0     0.064040  0.076845  ... -0.020753 -0.018140 -0.015530 -0.012924   \n",
      "1     0.064035  0.076839  ... -0.020919 -0.018292 -0.015667 -0.013043   \n",
      "2     0.064031  0.076834  ... -0.021070 -0.018431 -0.015791 -0.013152   \n",
      "3     0.064027  0.076830  ... -0.021208 -0.018558 -0.015905 -0.013251   \n",
      "4     0.064024  0.076826  ... -0.021334 -0.018674 -0.016009 -0.013342   \n",
      "...        ...       ...  ...       ...       ...       ...       ...   \n",
      "8215  0.099800  0.119665  ... -0.158858 -0.139152 -0.119391 -0.099580   \n",
      "8216  0.101834  0.122099  ... -0.162536 -0.142343 -0.122102 -0.101819   \n",
      "8217  0.103841  0.124502  ... -0.166052 -0.145392 -0.124692 -0.103957   \n",
      "8218  0.063966  0.076758  ... -0.024047 -0.021170 -0.018257 -0.015305   \n",
      "8219  0.064785  0.077739  ... -0.035654 -0.031385 -0.027061 -0.022684   \n",
      "\n",
      "           159       160       161       162  163  164  \n",
      "0    -0.010324 -0.007730 -0.005144 -0.002567  0.0    0  \n",
      "1    -0.010423 -0.007808 -0.005198 -0.002595  0.0    0  \n",
      "2    -0.010514 -0.007878 -0.005247 -0.002620  0.0    0  \n",
      "3    -0.010597 -0.007943 -0.005292 -0.002644  0.0    0  \n",
      "4    -0.010672 -0.008003 -0.005333 -0.002665  0.0    0  \n",
      "...        ...       ...       ...       ...  ...  ...  \n",
      "8215 -0.079727 -0.059837 -0.039915 -0.019968  0.0    0  \n",
      "8216 -0.081502 -0.061155 -0.040786 -0.020399  0.0    0  \n",
      "8217 -0.083196 -0.062413 -0.041616 -0.020809  0.0    0  \n",
      "8218 -0.012318 -0.009293 -0.006232 -0.003134  0.0    0  \n",
      "8219 -0.018253 -0.013769 -0.009232 -0.004642  0.0    0  \n",
      "\n",
      "[8220 rows x 165 columns]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = df = pd.DataFrame()\n",
    "Data_point = \"Data\"\n",
    "d = pd.read_csv(\"Data_folder/\" + str(Data_point) + \".csv\", header=None)\n",
    "\n",
    "st = d[0]\n",
    "volume0 = d[1]\n",
    "rneedle = d[2]\n",
    "rr_zz = d.loc[0][3:]\n",
    "\n",
    "data_matrix = data_matrix.append(d, ignore_index=True)\n",
    "\n",
    "print(data_matrix)\n",
    "train_loader = []\n",
    "val_loader = []\n",
    "test_loader = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deside to apply host or device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal_per_epoch(loss_list, model, loader) -> list:\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "            loss_epoch += loss\n",
    "        loss_list.append(loss_epoch/t)\n",
    "\n",
    "def train(model, optimizer, epochs=1):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_cal_per_epoch(loss_train, model, train_loader)\n",
    "        loss_cal_per_epoch(loss_val, model, val_loader)\n",
    "        print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "    \n",
    "    x = range(1, epochs+1)\n",
    "    plt.plot(x, loss_train, 'b-', label=\"training loss\")\n",
    "    plt.plot(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def prediction(model, x):   # only a batch\n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        score = torch.Tensor.cpu(model(x))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64def4006c78149665c79cf5850ee76c9e416630a0d9e75e41ff194dcaf5fb2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
