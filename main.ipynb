{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for tensiometry of pendant drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and self-defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "#Test/Train data split\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import logging\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "# os\n",
    "import os\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# self-defined model\n",
    "from model_pic import *\n",
    "from model_cal import *\n",
    "import pandas as pd\n",
    "\n",
    "#PictureNet\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self, dataset, test_train_split=0.8, val_train_split=0.1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = list(range(dataset_size))\n",
    "        test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        train_indices, self.test_indices = self.indices[:test_split], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[:validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "    \n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01319281  0.02999143  0.02705778 ... -0.01370682 -0.00034966\n",
      "   0.01379137]\n",
      " [ 0.00801515  0.01553567  0.04356342 ... -0.01458989 -0.00972398\n",
      "  -0.00011938]\n",
      " [-0.0013863   0.01228125  0.02536321 ... -0.01401782 -0.01238109\n",
      "   0.01033983]\n",
      " ...\n",
      " [ 0.00161995  0.03090525  0.03784142 ... -0.05787808 -0.02906607\n",
      "   0.0178563 ]\n",
      " [ 0.00219871  0.02548508  0.05479375 ... -0.0567024  -0.01094999\n",
      "   0.00627673]\n",
      " [ 0.00201977  0.03208792  0.0523237  ... -0.052684   -0.02069526\n",
      "   0.00432294]]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = df = pd.DataFrame()\n",
    "\n",
    "data_matrix = pd.read_csv(\"Data_folder/Data_big_drop.csv\", header= None)\n",
    "\n",
    "#print(data_matrix)\n",
    "\n",
    "\n",
    "# Make the droplet dataset class based on data_matrix\n",
    "class Droplet_data_set(Dataset):\n",
    "    def __init__(self):\n",
    "        x = data_matrix.iloc[0:,3:].values\n",
    "        y = data_matrix.iloc[0:,0:2].values\n",
    "        x = np.random.normal(x,0.01)\n",
    "        # Add normalization for x\n",
    "        #x = preprocessing.normalize(x)\n",
    "        print(x)\n",
    "\n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "         return len(self.y_train)\n",
    "        \n",
    "my_data_set = Droplet_data_set()\n",
    "\n",
    "# Split the single dataset into 3 datasets for training, test and validation.\n",
    "split = DataSplit(my_data_set, shuffle=True)\n",
    "train_loader, val_loader, test_loader = split.get_split(batch_size=64, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for ImageNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((25, 479, 586), (25, 160)), ((3, 479, 586), (3, 160)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_img = []\n",
    "\n",
    "data_image = pd.read_csv(\"Data_folder/Picture.csv\", header= None)\n",
    "\n",
    "for img_name in data_image.iloc[0:,3:].index + 1:\n",
    "    # defining the image path\n",
    "    image_path = 'Data_folder/' + str(img_name) + '.png'\n",
    "    # reading the image\n",
    "    img = imread(image_path, as_gray=True)\n",
    "    img = img[4:-4, 4:-4]  #crop \n",
    "    # normalizing the pixel values\n",
    "    img /= 255.0\n",
    "    # converting the type of pixel to float 32\n",
    "    img = img.astype('float32')\n",
    "    # appending the image into the list\n",
    "    train_img.append(img)\n",
    "    \n",
    "plt.imshow(train_img[0], cmap='gray')\n",
    "# converting the list to numpy array\n",
    "train_x_pic = np.array(train_img)\n",
    "train_y_pic = data_image.iloc[0:,3:].values\n",
    "train_x_pic, val_x_pic, train_y_pic, val_y_pic = train_test_split(train_x_pic, train_y_pic, test_size = 0.1)\n",
    "(train_x_pic.shape, train_y_pic.shape), (val_x_pic.shape, val_y_pic.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 479, 586]), torch.Size([2, 160]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the target into torch format\n",
    "train_y_pic = train_y_pic.astype(int);\n",
    "train_y_pic = torch.from_numpy(train_y_pic)\n",
    "\n",
    "train_x_pic = train_x_pic.reshape(25, 1, 479, 586)\n",
    "train_x_pic  = torch.from_numpy(train_x_pic)\n",
    "\n",
    "train_y_pic.shape, train_x_pic.shape\n",
    "\n",
    "# converting validation images into torch format\n",
    "val_x_pic = val_x_pic.reshape(3, 1, 479, 586)\n",
    "val_x_pic  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y_pic = val_y.astype(int);\n",
    "val_y_pic = torch.from_numpy(val_y)\n",
    "\n",
    "# shape of validation data\n",
    "val_x_pic.shape, val_y_pic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deside to apply host or device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal_per_epoch(loss_list, model, loader):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "            loss_epoch += loss\n",
    "        loss_list.append(loss_epoch/t)\n",
    "\n",
    "def train(model, optimizer, epochs=1):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for _, (x, y) in enumerate(train_loader):\n",
    "            x = x.reshape(len(x), 1, 1, -1)     # reshape to 4D data for formal input of model\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_cal_per_epoch(loss_train, model, train_loader)\n",
    "        loss_cal_per_epoch(loss_val, model, val_loader)\n",
    "        print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "    \n",
    "    x = range(1, epochs+1)\n",
    "\n",
    "    plt.plot(x, loss_train, 'b-', label=\"training loss\")\n",
    "    plt.plot(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def prediction(model, x):   # only a batch\n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        score = torch.Tensor.cpu(model(x))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PhysicsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_loss = 19.6892, val_loss = 29.8582\n",
      "epoch 2: train_loss = 6.1559, val_loss = 10.7513\n",
      "epoch 3: train_loss = 4.1957, val_loss = 6.4638\n",
      "epoch 4: train_loss = 2.9718, val_loss = 4.7152\n",
      "epoch 5: train_loss = 0.8011, val_loss = 1.1282\n",
      "epoch 6: train_loss = 0.9517, val_loss = 1.8216\n",
      "epoch 7: train_loss = 1.1706, val_loss = 1.8463\n",
      "epoch 8: train_loss = 1.1218, val_loss = 1.4373\n",
      "epoch 9: train_loss = 0.8172, val_loss = 1.6976\n",
      "epoch 10: train_loss = 0.4158, val_loss = 0.3702\n",
      "epoch 11: train_loss = 0.3874, val_loss = 0.4503\n",
      "epoch 12: train_loss = 0.3271, val_loss = 0.5344\n",
      "epoch 13: train_loss = 0.4376, val_loss = 0.5190\n",
      "epoch 14: train_loss = 1.3397, val_loss = 1.6451\n",
      "epoch 15: train_loss = 0.3886, val_loss = 0.3848\n",
      "epoch 16: train_loss = 0.3422, val_loss = 0.3420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18780/3389110802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_physics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mPATH_CAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_cal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_physics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH_CAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18780/3416907892.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, epochs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloss_cal_per_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\envs\\m14sc\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\envs\\m14sc\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_channel = 160\n",
    "node_1 = 512\n",
    "node_2 = 256\n",
    "node_3 = 128\n",
    "node_4 = 64\n",
    "out_channel = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_physics = PhysicsNet(in_channel, node_1, node_2, node_3, node_4, out_channel)\n",
    "optimizer = optim.Adam(model_physics.parameters(), lr=learning_rate)\n",
    "epoch = 100\n",
    "\n",
    "train(model_physics, optimizer, epoch)\n",
    "PATH_CAL = os.path.join(os.getcwd(), \"model_cal\")\n",
    "torch.save(model_physics, PATH_CAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PictureNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####To be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truths = tensor([[ 9.8000,  3.9000],\n",
      "        [ 5.7000,  1.7000],\n",
      "        [ 7.7000,  2.5000],\n",
      "        [11.6000,  3.7000],\n",
      "        [ 4.5000,  1.2000],\n",
      "        [ 9.8000,  3.4000],\n",
      "        [ 5.9000,  2.0000],\n",
      "        [11.4000,  3.1000]])\n",
      "Prediction    = tensor([[10.3801,  4.2324],\n",
      "        [ 5.0403,  1.7249],\n",
      "        [ 7.1923,  2.3698],\n",
      "        [11.5871,  4.0177],\n",
      "        [ 3.3756,  1.2351],\n",
      "        [10.6139,  3.6268],\n",
      "        [ 5.0317,  1.9173],\n",
      "        [11.0337,  3.4015]])\n"
     ]
    }
   ],
   "source": [
    "model_trained = torch.load(PATH_CAL)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "_, (x_test_example, y_test_example) = next(examples)\n",
    "\n",
    "score_example = prediction(model_trained, x_test_example)\n",
    "\n",
    "# Print only 8 data samples for comparison\n",
    "print(\"Ground Truths =\", y_test_example[:8])\n",
    "print(\"Prediction    =\", score_example[:8].reshape(8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64def4006c78149665c79cf5850ee76c9e416630a0d9e75e41ff194dcaf5fb2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
