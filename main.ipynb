{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for tensiometry of pendant drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and self-defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "#Test/Train data split\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import logging\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "# os\n",
    "import os\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# self-defined model\n",
    "from model_pic import *\n",
    "from model_cal import *\n",
    "import pandas as pd\n",
    "\n",
    "#PictureNet\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "\n",
    "    def __init__(self, dataset, test_train_split=0.8, val_train_split=0.1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = list(range(dataset_size))\n",
    "        test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        train_indices, self.test_indices = self.indices[:test_split], self.indices[test_split:]\n",
    "        train_size = len(train_indices)\n",
    "        validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "\n",
    "        self.train_indices, self.val_indices = train_indices[:validation_split], train_indices[validation_split:]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "    \n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_split(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=batch_size, num_workers=num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_train_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.train_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.train_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.val_loader\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def get_test_loader(self, batch_size=50, num_workers=0):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=self.test_sampler, shuffle=False, num_workers=num_workers)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for PhysicsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = df = pd.DataFrame()\n",
    "\n",
    "data_matrix = pd.read_csv(\"Data_folder/Picture.csv\", header= None)\n",
    "\n",
    "#print(data_matrix)\n",
    "\n",
    "\n",
    "# Make the droplet dataset class based on data_matrix\n",
    "class Droplet_data_set(Dataset):\n",
    "    def __init__(self):\n",
    "        x = data_matrix.iloc[0:,3:].values\n",
    "        y = data_matrix.iloc[0:,0:2].values\n",
    "        # x = np.random.normal(x,0.01)\n",
    "        y0 = data_matrix.iloc[:,0].values\n",
    "        y1 = data_matrix.iloc[:,1].values\n",
    "        \n",
    "        # Add normalization for x\n",
    "        # x = preprocessing.normalize(x)\n",
    "        # y = preprocessing.normalize(y, axis = 0,norm='l2')\n",
    "\n",
    "\n",
    "        self.x_train = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "         return len(self.y_train)\n",
    "        \n",
    "my_data_set = Droplet_data_set()\n",
    "\n",
    "# Split the single dataset into 3 datasets for training, test and validation.\n",
    "split = DataSplit(my_data_set, shuffle=True)\n",
    "train_loader, val_loader, test_loader = split.get_split(batch_size=64, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for PictureNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_img = []\n",
    "\n",
    "data_image = pd.read_csv(\"Data_folder/Picture.csv\", header= None)\n",
    "for img_name in data_image.iloc[0:32,3:].index + 1:\n",
    "    # defining the image path\n",
    "    image_path = 'Data_folder/' + str(img_name) + '.png'\n",
    "    # reading the image\n",
    "    img = imread(image_path, as_gray=True)\n",
    "    img = img[4:-4, 4:-4]  #crop \n",
    "    # normalizing the pixel values\n",
    "    img /= 255.0\n",
    "    # converting the type of pixel to float 32\n",
    "    img = img.astype('float32')\n",
    "    # appending the image into the list\n",
    "    train_img.append(img)\n",
    "    \n",
    "plt.imshow(train_img[0], cmap='gray')\n",
    "\n",
    "# converting the list to numpy array\n",
    "train_x_pic = np.array(train_img)\n",
    "train_y_pic = data_image.iloc[0:32,3:].values\n",
    "train_x_pic, rem_x_pic, train_y_pic, rem_y_pic = train_test_split(train_x_pic, train_y_pic, test_size = 0.2)\n",
    "val_x_pic, test_x_pic, val_y_pic, test_y_pic = train_test_split(rem_x_pic, rem_y_pic, test_size = 0.5)\n",
    "# (train_x_pic.shape, train_y_pic.shape), (val_x_pic.shape, val_y_pic.shape), (test_x_pic.shape, test_y_pic.shape)\n",
    "\n",
    "# converting the trian images and targets into torch format\n",
    "train_x_pic = train_x_pic.reshape(len(train_x_pic), 1, 82, 77)\n",
    "train_x_pic  = torch.from_numpy(train_x_pic)\n",
    "train_y_pic = train_y_pic.astype(int)\n",
    "train_y_pic = torch.from_numpy(train_y_pic)\n",
    "# train_y_pic.shape, train_x_pic.shape\n",
    "\n",
    "# converting the val images and targets into torch format\n",
    "val_x_pic = val_x_pic.reshape(len(val_x_pic), 1, 82, 77)\n",
    "val_x_pic  = torch.from_numpy(val_x_pic)\n",
    "val_y_pic = val_y_pic.astype(int)\n",
    "val_y_pic = torch.from_numpy(val_y_pic)\n",
    "# val_x_pic.shape, val_y_pic.shape\n",
    "\n",
    "# converting the test images and targets into torch format\n",
    "test_x_pic = test_x_pic.reshape(len(test_x_pic), 1, 82, 77)\n",
    "test_x_pic  = torch.from_numpy(test_x_pic)\n",
    "test_y_pic = test_y_pic.astype(int)\n",
    "test_y_pic = torch.from_numpy(test_y_pic)\n",
    "# test_x_pic.shape, test_y_pic.shape\n",
    "\n",
    "# converting the torch into data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deside to apply host or device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal_per_epoch(loss_list, model, loader):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "            loss_epoch += loss\n",
    "        loss_list.append(loss_epoch/t)\n",
    "\n",
    "def prediction(model, x):   # only a batch\n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        score = torch.Tensor.cpu(model(x))\n",
    "    return score\n",
    "\n",
    "def train_physics(model, optimizer, epochs=1):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for _, (x, y) in enumerate(train_loader):\n",
    "            x = x.reshape(len(x), 1, 1, -1)     # reshape to 4D data for formal input of model\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_cal_per_epoch(loss_train, model, train_loader)\n",
    "        loss_cal_per_epoch(loss_val, model, val_loader)\n",
    "        print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "    \n",
    "    x = range(1, epochs+1)\n",
    "\n",
    "    plt.semilogy(x, loss_train, 'b-', label=\"training loss\")\n",
    "    plt.semilogy(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_picture(model, optimizer, epochs=1):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for (x, y) in (train_x_pic, train_y_pic):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float32)\n",
    "            # testing \n",
    "            print(x.shape, y.shape)\n",
    "            ####\n",
    "            scores = model(x)\n",
    "            loss = F.mse_loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # loss_cal_per_epoch(loss_train, model, train_loader)\n",
    "        # loss_cal_per_epoch(loss_val, model, val_loader)\n",
    "        # print('epoch %d: train_loss = %.4f, val_loss = %.4f' % (e+1, loss_train[e], loss_val[e]))\n",
    "    \n",
    "    # x = range(1, epochs+1)\n",
    "\n",
    "    # plt.semilogy(x, loss_train, 'b-', label=\"training loss\")\n",
    "    # plt.semilogy(x, loss_val, 'r--', label=\"validation loss\")\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"MSE Loss\")\n",
    "    # plt.title(\"Train/Validation Loss\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PhysicsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_loss = 13.1256, val_loss = 13.2607\n",
      "epoch 2: train_loss = 6.7906, val_loss = 6.6151\n",
      "epoch 3: train_loss = 11.0585, val_loss = 12.5253\n",
      "epoch 4: train_loss = 5.2082, val_loss = 6.0727\n",
      "epoch 5: train_loss = 1.4248, val_loss = 1.4897\n",
      "epoch 6: train_loss = 7.0046, val_loss = 7.5810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jrimm\\Documents\\tue\\DL_Pendant_Drop_Tensiometry\\main.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000014?line=10'>11</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_physics\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000014?line=11'>12</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000014?line=13'>14</a>\u001b[0m train_physics(model_physics, optimizer, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000014?line=14'>15</a>\u001b[0m PATH_CAL \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m\"\u001b[39m\u001b[39mmodel_cal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000014?line=15'>16</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model_physics, PATH_CAL)\n",
      "\u001b[1;32mc:\\Users\\jrimm\\Documents\\tue\\DL_Pendant_Drop_Tensiometry\\main.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain_physics\u001b[1;34m(model, optimizer, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(x), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)     \u001b[39m# reshape to 4D data for formal input of model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()  \u001b[39m# put model to training mode\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=28'>29</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)  \u001b[39m# move to device, e.g. GPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=29'>30</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=30'>31</a>\u001b[0m scores \u001b[39m=\u001b[39m model(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_channel = 160\n",
    "node_1 = 4096\n",
    "node_2 = 4096\n",
    "node_3 = 1024\n",
    "node_4 = 512\n",
    "out_channel = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_physics = PhysicsNet(in_channel, node_1, node_2, node_3, node_4, out_channel)\n",
    "optimizer = optim.Adam(model_physics.parameters(), lr=learning_rate)\n",
    "epoch = 200\n",
    "\n",
    "train_physics(model_physics, optimizer, epoch)\n",
    "PATH_CAL = os.path.join(os.getcwd(), \"model_cal\")\n",
    "torch.save(model_physics, PATH_CAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truths = tensor([[39.6000, 12.6000],\n",
      "        [39.0000, 12.2000],\n",
      "        [38.4000, 12.3000],\n",
      "        [35.3000, 13.7000],\n",
      "        [36.9000, 10.4000],\n",
      "        [40.7000, 19.0000],\n",
      "        [39.6000, 16.0000],\n",
      "        [31.7000, 12.1000],\n",
      "        [33.4000, 11.5000],\n",
      "        [36.1000, 12.2000],\n",
      "        [33.9000, 14.7000],\n",
      "        [41.1000, 19.5000],\n",
      "        [38.6000, 16.1000],\n",
      "        [32.5000, 10.8000],\n",
      "        [35.2000, 14.8000]])\n",
      "Prediction    = tensor([[41.9702, 13.0229],\n",
      "        [41.1339, 12.4790],\n",
      "        [40.6482, 12.6433],\n",
      "        [37.3277, 13.9196],\n",
      "        [39.0698, 10.6952],\n",
      "        [39.9571, 18.4248],\n",
      "        [41.6939, 16.0780],\n",
      "        [33.7561, 12.1455],\n",
      "        [35.3777, 11.4258],\n",
      "        [38.2097, 12.4473],\n",
      "        [35.1381, 14.7583],\n",
      "        [38.8045, 18.6697],\n",
      "        [40.3051, 16.1429],\n",
      "        [34.5094, 10.6821],\n",
      "        [36.6502, 14.7701]])\n"
     ]
    }
   ],
   "source": [
    "model_trained = torch.load(PATH_CAL)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "_, (x_test_example, y_test_example) = next(examples)\n",
    "\n",
    "score_example = prediction(model_trained, x_test_example)\n",
    "y = data_matrix.iloc[0:,0:2].values\n",
    "#Revert normalized values back to real values\n",
    "# Print only 8 data samples for comparison\n",
    "print(\"Ground Truths =\", y_test_example[:15])\n",
    "print(\"Prediction    =\", score_example[:15].reshape(15, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PictureNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jrimm\\Documents\\tue\\DL_Pendant_Drop_Tensiometry\\main.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000019?line=11'>12</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_picture\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000019?line=12'>13</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000019?line=14'>15</a>\u001b[0m train_picture(model_picture, optimizer, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000019?line=15'>16</a>\u001b[0m PATH_CAL \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m\"\u001b[39m\u001b[39mmodel_cal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000019?line=16'>17</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model_picture, PATH_CAL)\n",
      "\u001b[1;32mc:\\Users\\jrimm\\Documents\\tue\\DL_Pendant_Drop_Tensiometry\\main.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain_picture\u001b[1;34m(model, optimizer, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=53'>54</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)  \u001b[39m# move the model parameters to CPU/GPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=55'>56</a>\u001b[0m     \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m (train_x_pic, train_y_pic):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=56'>57</a>\u001b[0m         model\u001b[39m.\u001b[39mtrain()  \u001b[39m# put model to training mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jrimm/Documents/tue/DL_Pendant_Drop_Tensiometry/main.ipynb#ch0000012?line=57'>58</a>\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)  \u001b[39m# move to device, e.g. GPU\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "in_channel = 1\n",
    "channel_1 = 4\n",
    "channel_2 = 8\n",
    "channel_3 = 16\n",
    "node_1 = 4096\n",
    "node_2 = 4096\n",
    "out_channel = 160\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_picture = PictureNet(in_channel, channel_1, channel_2, channel_3, node_1, node_2, out_channel)\n",
    "optimizer = optim.Adam(model_picture.parameters(), lr=learning_rate)\n",
    "epoch = 1\n",
    "\n",
    "train_picture(model_picture, optimizer, epoch)\n",
    "PATH_CAL = os.path.join(os.getcwd(), \"model_cal\")\n",
    "torch.save(model_picture, PATH_CAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64def4006c78149665c79cf5850ee76c9e416630a0d9e75e41ff194dcaf5fb2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
